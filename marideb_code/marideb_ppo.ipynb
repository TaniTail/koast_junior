{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class WasteCollectionEnv(gym.Env):\n",
    "    \"\"\"해양 쓰레기 수거 환경\"\"\"\n",
    "    \n",
    "    def __init__(self, lon_mesh, lat_mesh, initial_waste, U_interp, V_interp, ocean_mask):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lon_mesh = lon_mesh\n",
    "        self.lat_mesh = lat_mesh\n",
    "        self.initial_waste = initial_waste\n",
    "        self.U_interp = U_interp\n",
    "        self.V_interp = V_interp\n",
    "        self.ocean_mask = ocean_mask\n",
    "        \n",
    "        # 상태 공간: [현재위치_x, 현재위치_y, 주변_쓰레기_분포(8방향), 해류_U, 해류_V]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([lon_mesh.min(), lat_mesh.min()] + [0]*8 + [-1, -1]),\n",
    "            high=np.array([lon_mesh.max(), lat_mesh.max()] + [1]*8 + [1, 1]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # 행동 공간: [이동방향(각도), 이동속도]\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([-np.pi, 0]),\n",
    "            high=np.array([np.pi, 1]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # 환경 파라미터\n",
    "        self.max_steps = 100\n",
    "        self.dt = 1  # 시간 간격 (시간)\n",
    "        self.collection_radius = 0.1  # 수거 반경 (도)\n",
    "        self.fuel_consumption_rate = 0.01  # 연료 소비율\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"현재 상태 관측\"\"\"\n",
    "        # 현재 위치 인덱스\n",
    "        x_idx = np.argmin(np.abs(self.lon_mesh[0,:] - self.current_pos[0]))\n",
    "        y_idx = np.argmin(np.abs(self.lat_mesh[:,0] - self.current_pos[1]))\n",
    "        \n",
    "        # 주변 8방향 쓰레기 분포\n",
    "        surroundings = []\n",
    "        for dy in [-1, 0, 1]:\n",
    "            for dx in [-1, 0, 1]:\n",
    "                if dx == 0 and dy == 0:\n",
    "                    continue\n",
    "                try:\n",
    "                    surroundings.append(\n",
    "                        self.waste[\n",
    "                            max(0, min(y_idx + dy, self.waste.shape[0]-1)),\n",
    "                            max(0, min(x_idx + dx, self.waste.shape[1]-1))\n",
    "                        ]\n",
    "                    )\n",
    "                except IndexError:\n",
    "                    surroundings.append(0)\n",
    "        \n",
    "        # 현재 위치의 해류\n",
    "        current_u = self.U_interp[y_idx, x_idx]\n",
    "        current_v = self.V_interp[y_idx, x_idx]\n",
    "        \n",
    "        return np.array([\n",
    "            self.current_pos[0],\n",
    "            self.current_pos[1]\n",
    "        ] + surroundings + [\n",
    "            current_u,\n",
    "            current_v\n",
    "        ], dtype=np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"환경 진행\"\"\"\n",
    "        self.steps += 1\n",
    "        \n",
    "        # 행동 적용 (이동)\n",
    "        angle, speed = action\n",
    "        dx = speed * np.cos(angle) * self.dt\n",
    "        dy = speed * np.sin(angle) * self.dt\n",
    "        \n",
    "        new_pos = [\n",
    "            self.current_pos[0] + dx,\n",
    "            self.current_pos[1] + dy\n",
    "        ]\n",
    "        \n",
    "        # 경계 확인\n",
    "        new_pos[0] = np.clip(new_pos[0], self.lon_mesh.min(), self.lon_mesh.max())\n",
    "        new_pos[1] = np.clip(new_pos[1], self.lat_mesh.min(), self.lat_mesh.max())\n",
    "        \n",
    "        # 육지 충돌 확인\n",
    "        x_idx = np.argmin(np.abs(self.lon_mesh[0,:] - new_pos[0]))\n",
    "        y_idx = np.argmin(np.abs(self.lat_mesh[:,0] - new_pos[1]))\n",
    "        \n",
    "        if np.isnan(self.ocean_mask[y_idx, x_idx]):\n",
    "            # 육지 충돌 시 이동 취소\n",
    "            new_pos = self.current_pos\n",
    "        \n",
    "        self.current_pos = new_pos\n",
    "        \n",
    "        # 쓰레기 수거\n",
    "        collected_waste = self._collect_waste()\n",
    "        \n",
    "        # 쓰레기 이동 시뮬레이션\n",
    "        self._update_waste_distribution()\n",
    "        \n",
    "        # 보상 계산\n",
    "        reward = (\n",
    "            collected_waste * 100  # 수거한 쓰레기\n",
    "            - speed * self.fuel_consumption_rate  # 연료 소비\n",
    "            - 0.1  # 시간 페널티\n",
    "        )\n",
    "        \n",
    "        # 종료 조건\n",
    "        done = (self.steps >= self.max_steps)\n",
    "        \n",
    "        return self.get_state(), reward, done, False, {}\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"환경 초기화\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # 초기 위치 (랜덤)\n",
    "        valid_positions = np.where(~np.isnan(self.ocean_mask))\n",
    "        random_idx = np.random.randint(len(valid_positions[0]))\n",
    "        self.current_pos = [\n",
    "            self.lon_mesh[valid_positions[0][random_idx], valid_positions[1][random_idx]],\n",
    "            self.lat_mesh[valid_positions[0][random_idx], valid_positions[1][random_idx]]\n",
    "        ]\n",
    "        \n",
    "        # 쓰레기 분포 초기화\n",
    "        self.waste = self.initial_waste.copy()\n",
    "        self.steps = 0\n",
    "        \n",
    "        return self.get_state(), {}\n",
    "    \n",
    "    def _collect_waste(self):\n",
    "        \"\"\"현재 위치에서 쓰레기 수거\"\"\"\n",
    "        x_idx = np.argmin(np.abs(self.lon_mesh[0,:] - self.current_pos[0]))\n",
    "        y_idx = np.argmin(np.abs(self.lat_mesh[:,0] - self.current_pos[1]))\n",
    "        \n",
    "        # 수거 반경 내의 쓰레기 수거\n",
    "        collection_mask = np.zeros_like(self.waste)\n",
    "        for i in range(max(0, y_idx-1), min(self.waste.shape[0], y_idx+2)):\n",
    "            for j in range(max(0, x_idx-1), min(self.waste.shape[1], x_idx+2)):\n",
    "                if np.sqrt(\n",
    "                    (self.lon_mesh[i,j] - self.current_pos[0])**2 +\n",
    "                    (self.lat_mesh[i,j] - self.current_pos[1])**2\n",
    "                ) <= self.collection_radius:\n",
    "                    collection_mask[i,j] = 1\n",
    "        \n",
    "        collected = np.sum(self.waste * collection_mask)\n",
    "        self.waste *= (1 - collection_mask)\n",
    "        \n",
    "        return collected\n",
    "    \n",
    "    def _update_waste_distribution(self):\n",
    "        \"\"\"쓰레기 이동 시뮬레이션\"\"\"\n",
    "        waste_new = self.waste.copy()\n",
    "        \n",
    "        # 이류 항 계산\n",
    "        dx = 0.01\n",
    "        dy = 0.01\n",
    "        \n",
    "        waste_new[1:-1, 1:-1] -= (\n",
    "            (self.U_interp[1:-1, 1:-1] * \n",
    "             (self.waste[1:-1, 2:] - self.waste[1:-1, :-2]) / (2 * dx)) * self.dt\n",
    "        )\n",
    "        \n",
    "        waste_new[1:-1, 1:-1] -= (\n",
    "            (self.V_interp[1:-1, 1:-1] * \n",
    "             (self.waste[2:, 1:-1] - self.waste[:-2, 1:-1]) / (2 * dy)) * self.dt\n",
    "        )\n",
    "        \n",
    "        # 확산 항 계산\n",
    "        diffusion_coef = 0.01\n",
    "        waste_new[1:-1, 1:-1] += diffusion_coef * (\n",
    "            (self.waste[2:, 1:-1] + self.waste[:-2, 1:-1] + \n",
    "             self.waste[1:-1, 2:] + self.waste[1:-1, :-2] - \n",
    "             4 * self.waste[1:-1, 1:-1]) / (dx * dy)\n",
    "        ) * self.dt\n",
    "        \n",
    "        waste_new = waste_new * self.ocean_mask\n",
    "        waste_new[waste_new < 0] = 0\n",
    "        \n",
    "        self.waste = waste_new\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"PPO Actor 네트워크\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, action_dim * 2)  # 평균과 표준편차\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        output = self.network(state)\n",
    "        mean, log_std = output.chunk(2, dim=-1)\n",
    "        \n",
    "        # 각도는 -π ~ π로 제한\n",
    "        mean[:,0] = torch.tanh(mean[:,0]) * np.pi\n",
    "        # 속도는 0 ~ 1로 제한\n",
    "        mean[:,1] = torch.sigmoid(mean[:,1])\n",
    "        \n",
    "        # 표준편차는 양수\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        return mean, std\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"PPO Critic 네트워크\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "class PPO:\n",
    "    \"\"\"PPO 알고리즘\"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        \n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        \n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim)\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "        \n",
    "        self.clip_param = 0.2\n",
    "        self.max_grad_norm = 0.5\n",
    "        self.ppo_epochs = 10\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99\n",
    "        self.gae_lambda = 0.95\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        \"\"\"행동 선택\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            mean, std = self.actor(state)\n",
    "            dist = Normal(mean, std)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "            \n",
    "        return action.numpy()[0], log_prob.numpy()[0]\n",
    "    \n",
    "    def update(self, trajectories):\n",
    "        \"\"\"정책 업데이트\"\"\"\n",
    "        # 데이터 준비\n",
    "        states = torch.FloatTensor(np.array([t[0] for t in trajectories]))\n",
    "        actions = torch.FloatTensor(np.array([t[1] for t in trajectories]))\n",
    "        old_log_probs = torch.FloatTensor(np.array([t[2] for t in trajectories]))\n",
    "        rewards = torch.FloatTensor(np.array([t[3] for t in trajectories]))\n",
    "        next_states = torch.FloatTensor(np.array([t[4] for t in trajectories]))\n",
    "        dones = torch.FloatTensor(np.array([t[5] for t in trajectories]))\n",
    "        \n",
    "        # GAE 계산\n",
    "        with torch.no_grad():\n",
    "            values = self.critic(states)\n",
    "            next_values = self.critic(next_states)\n",
    "            \n",
    "            advantages = torch.zeros_like(rewards)\n",
    "            returns = torch.zeros_like(rewards)\n",
    "            advantage = 0\n",
    "            next_value = next_values[-1]\n",
    "            \n",
    "            for t in reversed(range(len(rewards))):\n",
    "                td_error = (\n",
    "                    rewards[t] +\n",
    "                    self.gamma * next_values[t] * (1 - dones[t]) -\n",
    "                    values[t]\n",
    "                )\n",
    "                advantage = (\n",
    "                    td_error +\n",
    "                    self.gamma * self.gae_lambda * (1 - dones[t]) * advantage\n",
    "                )\n",
    "                advantages[t] = advantage\n",
    "                returns[t] = advantage + values[t]\n",
    "        \n",
    "        # PPO 업데이트\n",
    "        for _ in range(self.ppo_epochs):\n",
    "            # 미니배치 샘플링\n",
    "            indices = np.random.permutation(len(trajectories))\n",
    "            \n",
    "            for start in range(0, len(trajectories), self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "                \n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                \n",
    "                # Actor 업데이트\n",
    "                mean, std = self.actor(batch_states)\n",
    "                dist = Normal(mean, std)\n",
    "                log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "                \n",
    "                ratio = torch.exp(log_probs - batch_old_log_probs)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marideb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
