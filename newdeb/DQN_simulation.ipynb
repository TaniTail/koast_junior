{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu118\n",
      "CUDA Available: True\n",
      "CUDA Version: 11.8\n",
      "GPU 개수: 1\n",
      "GPU 모델명: NVIDIA GeForce GT 1030\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)  \n",
    "print(\"CUDA Available:\", torch.cuda.is_available())  \n",
    "print(\"CUDA Version:\", torch.version.cuda)  \n",
    "print(\"GPU 개수:\", torch.cuda.device_count())  \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU 모델명:\", torch.cuda.get_device_name(0))  \n",
    "else:\n",
    "    print(\"❌ GPU를 사용할 수 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# 1️⃣ CUDA 설정 (GPU가 없으면 CPU 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2️⃣ 환경 정의 (MarineDebrisEnv)\n",
    "class MarineDebrisEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(MarineDebrisEnv, self).__init__()\n",
    "        \n",
    "        self.data = data\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # 상태(State) 정의: 선박 밀집도, 해류 속도, 해류 방향\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=100, shape=(4,), dtype=np.float32)\n",
    "        \n",
    "        # 행동(Action) 정의: 상하좌우 4가지 이동\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        \n",
    "        # 보상 설계: 쓰레기 밀집도가 높은 곳 방문 시 보상 증가\n",
    "        reward = self.data.iloc[self.current_step][\"Waste_Weight\"]\n",
    "        \n",
    "        return self._get_observation(), reward, done, {}\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        row = self.data.iloc[self.current_step]\n",
    "        return np.array([row[\"avg_ship_density\"], row[\"avg_ship_count\"], row[\"current_speed\"], row[\"current_dir\"]], dtype=np.float32)\n",
    "\n",
    "# 3️⃣ DQN 신경망 모델 (GPU 적용)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 4️⃣ DQN 학습 루프 (GPU 최적화 + 진행 상태 모니터링 추가)\n",
    "def train_dqn(env, episodes=500, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, lr=0.001):\n",
    "    model = DQN(input_dim=4, output_dim=4).to(device)  # 모델을 GPU로 이동\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    memory = deque(maxlen=10000)\n",
    "    batch_size = 256  # 배치 크기 증가하여 병렬 학습 최적화\n",
    "    \n",
    "    start_time = time.time()  # 전체 학습 시작 시간\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        episode_start = time.time()  # 개별 에피소드 시작 시간\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for _ in range(len(env.data)):\n",
    "            # 상태를 텐서로 변환 후 GPU로 이동\n",
    "            state_tensor = torch.FloatTensor(state).to(device)\n",
    "\n",
    "            # ε-greedy 정책 적용\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    action = torch.argmax(model(state_tensor)).item()\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            # 경험 재생 (Experience Replay) - GPU 최적화 적용\n",
    "            if len(memory) >= batch_size:\n",
    "                batch = random.sample(memory, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                # NumPy 변환 후 GPU로 이동\n",
    "                states = torch.FloatTensor(np.array(states)).to(device)\n",
    "                actions = torch.LongTensor(actions).to(device)\n",
    "                rewards = torch.FloatTensor(rewards).to(device)\n",
    "                next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "                dones = torch.FloatTensor(dones).to(device)\n",
    "\n",
    "                # Q-value 업데이트\n",
    "                current_q = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                next_q = model(next_states).max(1)[0].detach()\n",
    "                target_q = rewards + gamma * next_q * (1 - dones)\n",
    "\n",
    "                loss = loss_fn(current_q, target_q)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # ε 값 감소\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "        \n",
    "        # 🕒 진행 상태 출력 (에피소드별 실행 시간 + 총 경과 시간)\n",
    "        episode_time = time.time() - episode_start\n",
    "        total_elapsed = time.time() - start_time\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}, Time: {episode_time:.2f}s, Total Elapsed: {total_elapsed:.2f}s\")\n",
    "\n",
    "    print(f\"✅ 전체 학습 완료! 총 소요 시간: {time.time() - start_time:.2f}초\")\n",
    "    return model\n",
    "\n",
    "# 5️⃣ 데이터 로드 및 환경 설정\n",
    "data = pd.read_csv(\"marine_data.csv\")  # 데이터를 불러와 사용\n",
    "env = MarineDebrisEnv(data)\n",
    "\n",
    "# 6️⃣ 모델 훈련 시작\n",
    "trained_model = train_dqn(env)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sden",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
