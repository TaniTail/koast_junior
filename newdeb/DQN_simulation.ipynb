{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu118\n",
      "CUDA Available: True\n",
      "CUDA Version: 11.8\n",
      "GPU ê°œìˆ˜: 1\n",
      "GPU ëª¨ë¸ëª…: NVIDIA GeForce GT 1030\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)  \n",
    "print(\"CUDA Available:\", torch.cuda.is_available())  \n",
    "print(\"CUDA Version:\", torch.version.cuda)  \n",
    "print(\"GPU ê°œìˆ˜:\", torch.cuda.device_count())  \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU ëª¨ë¸ëª…:\", torch.cuda.get_device_name(0))  \n",
    "else:\n",
    "    print(\"âŒ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# 1ï¸âƒ£ CUDA ì„¤ì • (GPUê°€ ì—†ìœ¼ë©´ CPU ì‚¬ìš©)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2ï¸âƒ£ í™˜ê²½ ì •ì˜ (MarineDebrisEnv)\n",
    "class MarineDebrisEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(MarineDebrisEnv, self).__init__()\n",
    "        \n",
    "        self.data = data\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # ìƒíƒœ(State) ì •ì˜: ì„ ë°• ë°€ì§‘ë„, í•´ë¥˜ ì†ë„, í•´ë¥˜ ë°©í–¥\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=100, shape=(4,), dtype=np.float32)\n",
    "        \n",
    "        # í–‰ë™(Action) ì •ì˜: ìƒí•˜ì¢Œìš° 4ê°€ì§€ ì´ë™\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        \n",
    "        # ë³´ìƒ ì„¤ê³„: ì“°ë ˆê¸° ë°€ì§‘ë„ê°€ ë†’ì€ ê³³ ë°©ë¬¸ ì‹œ ë³´ìƒ ì¦ê°€\n",
    "        reward = self.data.iloc[self.current_step][\"Waste_Weight\"]\n",
    "        \n",
    "        return self._get_observation(), reward, done, {}\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        row = self.data.iloc[self.current_step]\n",
    "        return np.array([row[\"avg_ship_density\"], row[\"avg_ship_count\"], row[\"current_speed\"], row[\"current_dir\"]], dtype=np.float32)\n",
    "\n",
    "# 3ï¸âƒ£ DQN ì‹ ê²½ë§ ëª¨ë¸ (GPU ì ìš©)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 4ï¸âƒ£ DQN í•™ìŠµ ë£¨í”„ (GPU ìµœì í™” + ì§„í–‰ ìƒíƒœ ëª¨ë‹ˆí„°ë§ ì¶”ê°€)\n",
    "def train_dqn(env, episodes=500, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, lr=0.001):\n",
    "    model = DQN(input_dim=4, output_dim=4).to(device)  # ëª¨ë¸ì„ GPUë¡œ ì´ë™\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    memory = deque(maxlen=10000)\n",
    "    batch_size = 256  # ë°°ì¹˜ í¬ê¸° ì¦ê°€í•˜ì—¬ ë³‘ë ¬ í•™ìŠµ ìµœì í™”\n",
    "    \n",
    "    start_time = time.time()  # ì „ì²´ í•™ìŠµ ì‹œì‘ ì‹œê°„\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        episode_start = time.time()  # ê°œë³„ ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œê°„\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for _ in range(len(env.data)):\n",
    "            # ìƒíƒœë¥¼ í…ì„œë¡œ ë³€í™˜ í›„ GPUë¡œ ì´ë™\n",
    "            state_tensor = torch.FloatTensor(state).to(device)\n",
    "\n",
    "            # Îµ-greedy ì •ì±… ì ìš©\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    action = torch.argmax(model(state_tensor)).item()\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            # ê²½í—˜ ì¬ìƒ (Experience Replay) - GPU ìµœì í™” ì ìš©\n",
    "            if len(memory) >= batch_size:\n",
    "                batch = random.sample(memory, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                # NumPy ë³€í™˜ í›„ GPUë¡œ ì´ë™\n",
    "                states = torch.FloatTensor(np.array(states)).to(device)\n",
    "                actions = torch.LongTensor(actions).to(device)\n",
    "                rewards = torch.FloatTensor(rewards).to(device)\n",
    "                next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "                dones = torch.FloatTensor(dones).to(device)\n",
    "\n",
    "                # Q-value ì—…ë°ì´íŠ¸\n",
    "                current_q = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                next_q = model(next_states).max(1)[0].detach()\n",
    "                target_q = rewards + gamma * next_q * (1 - dones)\n",
    "\n",
    "                loss = loss_fn(current_q, target_q)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Îµ ê°’ ê°ì†Œ\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "        \n",
    "        # ğŸ•’ ì§„í–‰ ìƒíƒœ ì¶œë ¥ (ì—í”¼ì†Œë“œë³„ ì‹¤í–‰ ì‹œê°„ + ì´ ê²½ê³¼ ì‹œê°„)\n",
    "        episode_time = time.time() - episode_start\n",
    "        total_elapsed = time.time() - start_time\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}, Time: {episode_time:.2f}s, Total Elapsed: {total_elapsed:.2f}s\")\n",
    "\n",
    "    print(f\"âœ… ì „ì²´ í•™ìŠµ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ\")\n",
    "    return model\n",
    "\n",
    "# 5ï¸âƒ£ ë°ì´í„° ë¡œë“œ ë° í™˜ê²½ ì„¤ì •\n",
    "data = pd.read_csv(\"marine_data.csv\")  # ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ ì‚¬ìš©\n",
    "env = MarineDebrisEnv(data)\n",
    "\n",
    "# 6ï¸âƒ£ ëª¨ë¸ í›ˆë ¨ ì‹œì‘\n",
    "trained_model = train_dqn(env)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sden",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
